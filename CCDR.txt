CloudChaser: Architectural Master Specification and Implementation Strategy1. Executive Summary and System PhilosophyThe CloudChaser initiative represents a high-complexity convergence of edge computing, cross-platform mobile architecture, and multimodal artificial intelligence. The objective is to engineer a hybrid native application capable of real-time meteorological classification and analysis, specifically targeting the nuanced identification of cloud formations and the estimation of Liquid Water Content (LWC). This document serves as the comprehensive Software Requirements Specification (SRS) and Master Prompt for the "Google Anti Gravity" generative engine. It is designed to navigate the intricate dependency chains between Next.js 15, Capacitor, and the PyTorch/ONNX ecosystem while leveraging the semantic reasoning capabilities of Google’s Gemini 1.5 Flash model.The system architecture is predicated on a "Split-Brain" inference model. The primary cognitive layer resides on the edge, utilizing a quantized MobileNetV3-Small model running within the user's browser context via onnxruntime-web. This ensures zero-latency classification of cloud types—Cirriform, Cumuliform, Stratiform, and Stratocumuliform—regardless of network connectivity.1 The secondary cognitive layer, triggered for deep analysis, leverages the Gemini 1.5 Flash multimodal model. This cloud-based oracle provides "second opinions," analyzing visual texture and optical depth to estimate precipitation probability and cloud base height, grounded in retrieval-augmented generation (RAG) from the Qdrant vector database.3This report dissects every component of the stack, from the physical properties of the "Clouds-1500" dataset to the CSS "transparency hacks" required to render React components over a Capacitor camera stream. It addresses the critical compatibility friction between React Server Components (RSC) in Next.js 15 and the static export requirements of mobile hybrid applications, providing a robust roadmap for the "Google Anti Gravity" agent to execute without architectural regression.2. The Meteorological Domain: Clouds-1500 and Cloud PhysicsTo build an effective AI meteorologist, the system must be grounded in the specific radiometric and physical characteristics of the target domain. The CloudChaser AI is trained on and calibrated against the "Clouds-1500" dataset, a specialized corpus of horizon-oriented sky imagery.2.1 The Clouds-1500 Dataset ArchitectureThe "Clouds-1500" dataset is an extension of the earlier Clouds-1000 dataset, curated by researchers at the Federal University of Santa Catarina (UFSC) in Brazil.5 Unlike satellite imagery (top-down), this dataset consists of images captured by ground-based cameras pointed towards the horizon in north and south directions.5 This perspective introduces specific computer vision challenges, notably the "horizon distortion" effect where cloud features become compressed as they approach the vanishing point, and the varying illumination conditions caused by the sun's trajectory relative to the camera sensor.2.1.1 Taxonomy and Class DistributionThe dataset classifies sky conditions into five distinct categories based on solar radiation absorption characteristics. Understanding these classes is vital for the "Google Anti Gravity" agent to configure the model's output layer correctly:Class IDClass NamePhysical CharacteristicsRadiometric Significance0CirriformHigh-altitude, ice-crystal clouds. Thin, wispy structure.Low solar attenuation; high transparency.1CumuliformVertically developed, dense clouds. Sharp edges.High solar variability; potential for rapid shading.2StratiformLayered, uniform clouds. Flat bases.Consistent, moderate solar attenuation (diffuse light).3StratocumuliformHybrid characteristics; rolling masses or banks.Variable attenuation; often precursors to weather changes.4BackgroundTrees, buildings, power lines, and clear sky.Noise class; requires masking or exclusion.6The dataset is organized hierarchically, typically with a structure that separates metadata from the raw image files. Mendeley Data structures often include a root directory with subfolders for regions or dates.7 For "Clouds-1500," the directory structure likely segregates images by capture date or camera orientation (North/South), necessitating a recursive crawling strategy during the Extract-Transform-Load (ETL) phase.52.1.2 Annotation Format: Supervisely JSONA critical implementation detail is the annotation format. The "Clouds-1500" dataset was labeled using the Supervisely platform, which outputs annotations in a specific JSON schema.6 Unlike the simple YOLO format (class x y w h), Supervisely uses a complex nested structure defining objects as polygons (exterior and interior points).9The raw data presents a significant engineering hurdle: PyTorch and YOLO architectures generally require bounding boxes (rectangles), but the dataset provides polygons. The "Google Anti Gravity" agent must generate an ETL pipeline that mathematically transforms these polygons into minimal enclosing rectangles (bounding boxes) while normalizing the coordinates to the `` range required by YOLOv8.102.2 Cloud Physics and Liquid Water Content (LWC)Beyond simple classification, CloudChaser estimates the Liquid Water Content (LWC) of the observed clouds. This metric, expressed in grams per cubic meter ($g/m^3$), is a critical indicator of cloud density, potential turbulence, and precipitation risk. Since a standard RGB camera cannot directly measure LWC, the application infers this value using a lookup table derived from meteorological literature, which correlates visual classification with physical water density ranges.2.2.1 LWC Stratification by Cloud TypeResearch indicates that LWC varies by orders of magnitude between cloud families. The application's backend logic utilizes these ranges to provide context to the user 12:Cirriform (Ice Phase): These clouds are composed primarily of ice crystals rather than liquid droplets. Consequently, they exhibit extremely low LWC values, typically around 0.03 g/m³.12 The visual transparency of cirrus clouds correlates directly with this low water mass.Stratiform (Stable Liquid Phase): Stratus clouds are generally uniform layers. Their LWC is moderate and relatively constant, ranging from 0.25 to 0.30 g/m³.12 In these clouds, visibility is often reduced to several hundred meters, but turbulence is usually low.Cumuliform (Convective Liquid Phase): These clouds represent areas of active vertical convection. They hold the highest density of liquid water, ranging from 0.50 g/m³ in fair-weather cumulus to over 3.0 g/m³ in Cumulonimbus (Cb) towers.12 The core of a cumuliform cloud can contain "wet patches" where LWC spikes significantly, posing icing risks to aviation and indicating potential heavy rain.13Stratocumuliform (Hybrid Phase): These clouds exhibit characteristics of both stratus and cumulus, with LWC values typically falling in the intermediate range of 0.30 to 0.45 g/m³.122.2.2 Radiometric ImplicationsThe system also infers solar irradiance impact. Cumuliform clouds, due to their high LWC and vertical thickness, cause the most dramatic fluctuations in solar energy generation (ramp-rate events), while Stratiform clouds cause a steady, predictable drop in irradiance.6 This context is injected into the Gemini 1.5 Flash prompt to allow the AI to generate safety warnings for users (e.g., "High likelihood of solar obstruction; potential turbulence if flying").3. Data Engineering and ETL Pipeline SpecificationThe integrity of the machine learning model depends entirely on the quality of the data ingestion pipeline. The "Google Anti Gravity" agent must generate a Python-based ETL (Extract, Transform, Load) suite that bridges the gap between the Supervisely JSON format and the YOLOv8 training requirements.3.1 The Supervisely-to-YOLO Transformation LogicThe ETL pipeline must define a class SuperviselyConverter that handles the geometric transformation of annotations. The inputs are the dataset root directory and a meta.json file defining the project classes.93.1.1 Coordinate NormalizationSupervisely stores coordinates in absolute pixels. YOLO requires relative coordinates. The converter must read the size field (height, width) from the JSON and apply the following transformation for every object 10:Extract Polygon Points: Retrieve the list of exterior points $P = [(x_1, y_1), (x_2, y_2),..., (x_n, y_n)]$.Calculate Bounding Box: Determine the extremes of the polygon.$$x_{min} = \min(x_i), \quad x_{max} = \max(x_i)$$$$y_{min} = \min(y_i), \quad y_{max} = \max(y_i)$$Compute Center and Dimensions:$$w_{abs} = x_{max} - x_{min}$$$$h_{abs} = y_{max} - y_{min}$$$$x_{center\_abs} = x_{min} + \frac{w_{abs}}{2}$$$$y_{center\_abs} = y_{min} + \frac{h_{abs}}{2}$$Normalize:$$x_{yolo} = \frac{x_{center\_abs}}{ImageWidth}$$$$y_{yolo} = \frac{y_{center\_abs}}{ImageHeight}$$$$w_{yolo} = \frac{w_{abs}}{ImageWidth}$$$$h_{yolo} = \frac{h_{abs}}{ImageHeight}$$The output must be a .txt file for each image containing lines in the format: <class_id> <x_yolo> <y_yolo> <w_yolo> <h_yolo>.183.1.2 Handling the "Background" ClassThe dataset includes a "Background" class.6 In object detection frameworks like YOLO, "background" is usually implicit (the absence of a bounding box). However, the CloudChaser specification requires explicit handling.Strategy: If the "Background" class refers to specific obstacles like trees or buildings, these should be mapped to a specific class ID (e.g., ID 4) so the app can render a "mask" or warning over them in the AR view.20 If "Background" merely refers to clear sky, those annotations should be ignored to allow the model to treat them as the null class. The "Google Anti Gravity" agent must inspect the meta.json to determine the nature of the background class and configure the mapping dictionary accordingly.3.2 Dataset Splitting and ValidationTo ensure robust model performance, the pipeline must implement a stratified split strategy.Ratio: 80% Training, 20% Validation.Stratification: The split must maintain the distribution of cloud classes found in the full dataset. Since "Clouds-1500" likely has class imbalances (e.g., fewer Cumuliform images in winter months), a random split could lead to validation sets lacking specific rare cloud types.5 The script must count class occurrences and sample proportionally.4. Computer Vision Architecture: MobileNetV3 and ONNXFor the client-side inference engine, CloudChaser utilizes MobileNetV3-Small. This architecture is explicitly designed for mobile CPUs, utilizing efficient "inverted residual" blocks and linear bottlenecks to minimize parameter count while maximizing accuracy.14.1 Model Configuration and TrainingThe training script (train_cloud_net.py) utilizes PyTorch. The model is instantiated using torchvision.models.mobilenet_v3_small(pretrained=True) to leverage transfer learning from ImageNet.1Classifier Modification: The default classifier head of MobileNetV3 (1000 classes) is replaced with a custom nn.Sequential block ending in a Linear layer with 5 outputs (Cirriform, Cumuliform, Stratiform, Stratocumuliform, Background).23Input Resolution: The model is trained on 224x224 pixel images. The data loader must implement resizing and center-cropping to match this input requirement.4.2 The Hardsigmoid Export ChallengeA critical technical risk identified in the research is the compatibility of the Hardsigmoid activation function with ONNX export. MobileNetV3 heavily utilizes Hardsigmoid and Hardswish activations for efficiency. However, older ONNX opsets (versions 9-11) do not natively support these operators, leading to RuntimeError: Exporting the operator hardsigmoid to ONNX opset version 9 is not supported.244.2.1 Remediation StrategyThe "Google Anti Gravity" agent must implement a two-pronged solution in the export_model.py script:Opset Versioning: The export function must explicitly specify opset_version=14 (or higher). This version includes native support for Hardsigmoid and Hardswish.Pythontorch.onnx.export(
    model,
    dummy_input,
    "cloudchaser_v1.onnx",
    opset_version=14,
    input_names=["input"],
    output_names=["output"],
   ...
)
Operator Replacement (Fallback): If the target onnxruntime-web version on the client supports only older opsets (a risk in some legacy WebViews), the script must patch the model definition to replace nn.Hardsigmoid with a functionally equivalent composite of standard operators (Relu6, Add, Mul, Div) prior to export.244.3 Client-Side Inference OptimizationThe frontend utilizes onnxruntime-web to execute the .onnx model.Execution Provider: The application initializes the session with executionProviders: ['webgl', 'wasm'].2 WebGL provides GPU acceleration for matrix operations, which is crucial for maintaining a smooth framerate in the AR view.Preprocessing: The raw video feed from the Capacitor camera arrives as an HTMLVideoElement or ImageData. This must be resized to 224x224, normalized (Mean: [0.485, 0.456, 0.406], Std: [0.229, 0.224, 0.225]), and transposed to "CHW" (Channel, Height, Width) layout before being converted to a generic Float32Array tensor.25 This preprocessing step is computationally expensive in JavaScript; utilizing a WebGL shader for the resizing and normalization step is a recommended optimization the agent should consider.265. Generative AI Cognitive Layer: Gemini 1.5 FlashWhile MobileNetV3 provides rapid classification, it lacks semantic understanding. To elevate CloudChaser from a simple classifier to an "AI Meteorologist," the system integrates Google Gemini 1.5 Flash.5.1 Why Gemini 1.5 Flash?Gemini 1.5 Flash is selected over the "Pro" variant due to its "knowledge distillation" architecture. It is optimized for high-volume, low-latency tasks, making it ideal for a mobile application where users expect near-instant feedback.4 Crucially, it is multimodal, capable of accepting both text prompts and raw image data in a single request, eliminating the need for a separate optical character recognition (OCR) or image captioning pipeline.275.2 Multimodal Prompt EngineeringThe "Master Prompt" specifies a complex instruction set for the Gemini API. The prompt is constructed dynamically in the Python backend (GeminiService class).5.2.1 The Prompt StructureThe request sent to Gemini combines:System Instruction: "You are a senior meteorologist specializing in cloud microphysics and aviation safety."Contextual Data: The LWC range and classification confidence score derived from the local MobileNetV3 model.Visual Input: The user's captured image (base64 encoded).User Query: "Analyze this cloud structure for potential turbulence."The code implementation uses the google-generativeai SDK:Pythonmodel = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content()
For larger images or session-based interactions, the agent utilizes the File API to upload the media first, then references the file URI in subsequent chat turns to reduce bandwidth overhead.275.3 Retrieval Augmented Generation (RAG) with QdrantTo prevent "hallucinations" (e.g., inventing incorrect LWC values), the system uses RAG.Vector Database: Qdrant is deployed as the knowledge store.29Ingestion: A collection named meteorology_papers is populated with chunked text from academic PDFs (e.g., "The Water Content of Cumuliform Cloud" 13) and the Clouds-1500 metadata.Retrieval Loop: When Gemini is queried, the backend first embeds the user's text, searches Qdrant for the top 3 relevant paragraphs, and injects them into the Gemini context window. This ensures the AI's response is cited and scientifically accurate.6. Frontend Architecture: Next.js 15 and Static ExportThe choice of Next.js 15 for a mobile app backend introduces specific constraints that deviate from standard web development. Capacitor requires the application to be a set of static assets (HTML, CSS, JS) that it wraps in a native web view.6.1 The Static Export ConstraintNext.js 15 is heavily optimized for Server-Side Rendering (SSR) and React Server Components. However, Capacitor cannot run a Node.js server on the phone. Therefore, the application must be configured for Static Site Generation (SSG).31Configuration: The next.config.mjs file must explicitly set output: 'export'.JavaScriptconst nextConfig = {
  output: 'export',
  images: { unoptimized: true }, // Essential for local images
  distDir: 'out'
};
The images: { unoptimized: true } setting is non-negotiable. Next.js's <Image> component defaults to calling an optimization API (/_next/image), which does not exist in the static export. Without this flag, all images in the app will fail to load.336.2 React 19 and Dependency ConflictsNext.js 15 relies on React 19 (RC or Beta). Many ecosystem libraries, specifically react-webcam (used for web testing) and UI libraries like ant-design, may define strict peer dependencies on React 18.35Resolution: The "Google Anti Gravity" agent must be instructed to use the --legacy-peer-deps flag during npm install to bypass these artificial version locks. Alternatively, the agent should recommend downgrading to Next.js 14 if stability is prioritized over using the latest Next.js 15 features, though the SRS mandate is for Next.js 15.6.3 Routing and Dynamic ParamsIn a static export, dynamic routes (e.g., /cloud/[id]) must be pre-rendered at build time using generateStaticParams.37 However, CloudChaser captures new unique clouds at runtime.Architecture Shift: The app cannot use file-system-based dynamic routing for user-captured content. Instead, it uses a Single Page Application (SPA) approach with client-side state management (Zustand or Redux) and query parameters (/analysis?id=123) which are handled by the client-side router, bypassing the need for pre-rendered HTML files for every possible cloud ID.7. Mobile Integration: Capacitor and the AR InterfaceThe "Jewel" of CloudChaser is its Augmented Reality (AR) interface, which overlays the compass and classification tags onto the live camera feed. This requires deep integration with the Capacitor runtime.7.1 The Capacitor Camera Preview "Black Screen" IssueA common failure mode in hybrid apps is the Camera Preview plugin rendering behind the WebView. Since the WebView usually has a white or black background, the camera is invisible.387.1.1 The Transparency HackTo reveal the camera, the HTML document must be made transparent. The "Google Anti Gravity" agent must implement a global CSS strategy:Global CSS:CSSbody.camera-active {
    background: transparent!important;
    --ion-background-color: transparent!important;
}
State Management: When the CameraView component mounts, it triggers a function to add the camera-active class to the document.body. When it unmounts, the class is removed. This ensures the rest of the app (settings, gallery) remains opaque and readable.397.2 Device Orientation and Compass OverlayTo provide directional context (e.g., "Cumuliform cloud approaching from the North"), the app uses the DeviceOrientation API.iOS 13+ Permissions: On iOS, access to the compass (alpha value) is protected. The app must implement a user-triggered permission flow:JavaScriptDeviceOrientationEvent.requestPermission()
   .then(response => { if (response === 'granted') {... } })
The agent must generate a specific UI component ("Enable Compass" button) that appears only on iOS devices to handle this requirement.417.3 Android Emulator NetworkingDeveloping the backend integration is complicated by the Android Emulator's networking isolation. The emulator does not see localhost as the host machine.The 10.0.2.2 Alias: The Capacitor config or the API client must detect the environment. If running on the Android Emulator, it must route API requests to http://10.0.2.2:8000 instead of http://localhost:8000. Failure to configure this results in Connection Refused errors.428. The Master Prompt for Google Anti GravityBased on the exhaustive analysis above, the following text is the formal Master Prompt to be issued to the AI coding agent. It encapsulates all architectural constraints, data engineering logic, and compatibility workarounds.System Identity: You are "Google Anti Gravity," a Principal Architect specializing in Hybrid Native AI Systems.Mission: Generate the complete source code and configuration ecosystem for CloudChaser, a Next.js 15 + Capacitor meteorological application.Phase 1: The Data Foundation (Python/ETL)You must generate a robust ETL pipeline script (etl_cloud1500.py) that handles the "Clouds-1500" dataset from Mendeley.Input Processing: Recursively scan the dataset directory structure (handling potential "North"/"South" subfolders).Supervisely Decoupling: Parse the JSON annotations. You must implement a geometric transformation function that converts the exterior polygon points into Normalized YOLO Bounding Boxes ($x_c, y_c, w, h$).Constraint: Verify that coordinates are clamped between 0 and 1.Class Mapping: strictly map the classes as follows:Cirriform -> 0Cumuliform -> 1Stratiform -> 2Stratocumuliform -> 3Background -> 4 (Ensure this class is retained for AR masking purposes).Splitting: Implement a Stratified Shuffle Split (80/20) to preserve class distribution balance in the validation set.Phase 2: The Vision Core (PyTorch/ONNX)Generate a training and export suite (model_engine.py).Architecture: Instantiate torchvision.models.mobilenet_v3_small(pretrained=True). Replace the classifier head with a 5-class output linear layer.The Hardsigmoid Fix: In the export_to_onnx function, you MUST set opset_version=14.Reasoning: This prevents the "Exporting operator hardsigmoid... not supported" error common in MobileNetV3 exports.Input Shape: Fix the input to (1, 3, 224, 224) to optimize memory usage in the web runtime.Phase 3: The Generative Backend (FastAPI/Gemini)Create a backend/main.py service.Gemini 1.5 Flash Integration: Use the google-generativeai SDK.Implement an endpoint /analyze that accepts a file upload (image) and metadata (local classification result).Prompt Engineering: Construct a system prompt that injects the following LWC context based on the local classification:If Cirriform: "Context: LWC ~0.03 g/m³. Risk: Low."If Cumuliform: "Context: LWC 0.5-3.0 g/m³. Risk: Turbulence/Precipitation."RAG Integration: Initialize a Qdrant client. Before calling Gemini, query the meteorology_papers collection for context relevant to the user's text query and append it to the Gemini prompt.CORS & Networking: Configure CORS to allow localhost:3000 and capacitor://localhost.Phase 4: The Hybrid Frontend (Next.js 15)Generate the Next.js application structure with strict adherence to Mobile export constraints.Next Config:JavaScriptconst nextConfig = {
  output: 'export',
  images: { unoptimized: true },
}
Dependency Management: Explicitly instruct the user to run npm install --legacy-peer-deps to resolve the conflict between Next.js 15 (React 19) and react-webcam/ant-design (React 18).The AR Camera Interface:Use @capacitor-community/camera-preview.The Transparency Hack: Create a utility useTransparentBody() hook. When the camera component mounts, it must add a class to document.body that sets background: transparent!important.Z-Index Layering: Create an Overlay component with z-index: 50 containing the Compass (SVG) and Inference Label.Client-Side Inference:Implement onnxruntime-web with the webgl execution provider.Create a Web Worker to handle image preprocessing (Resizing/Normalization) off the main thread to prevent UI jank.Phase 5: Capacitor ConfigurationNetworking: Create a helper function getApiUrl() that returns http://10.0.2.2:8000 if Capacitor.getPlatform() === 'android', and http://localhost:8000 otherwise.iOS Compass: Add a button in the Settings UI to trigger DeviceOrientationEvent.requestPermission() for iOS 13+ support.9. ConclusionThis Master Specification provides a complete blueprint for CloudChaser. It synthesizes the physical realities of the Clouds-1500 dataset with the architectural constraints of modern hybrid app development. By proactively addressing the Hardsigmoid ONNX error, the Next.js static export limitations, and the Capacitor transparency requirements, the "Google Anti Gravity" agent is positioned to generate a functional, production-grade application codebase on the first pass, avoiding the common pitfalls associated with this complex technology stack.Citations7 Mendeley Data Structure.6 Clouds-1000/1500 Classification & Supervisely Tool.1 MobileNetV3 Architecture.2 ONNX Runtime Web Integration.12 Cloud Liquid Water Content (LWC) Research.42 Android Emulator Networking (10.0.2.2).38 Capacitor Camera Preview Black Screen Issues.31 Next.js Static Export Configuration.4 Gemini 1.5 Flash Technical Details.24 ONNX Hardsigmoid Operator Compatibility.29 Qdrant Vector Database Integration.9 Supervisely to YOLO Conversion Logic.